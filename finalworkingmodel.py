# -*- coding: utf-8 -*-
"""FinalWorkingModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11ZuUcwNXVgzt5uWbSPCY3nomK8hBwJ8f
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from xgboost import XGBRegressor
from sklearn import metrics

df = pd.read_csv("/content/Train (1).csv")

df

df.shape

df.info()

# checking for missing values
df.isnull().sum()

# mean values of the "item_weight" column
df['Item_Weight'].mean()

# filling the missing values in "item_weight" column with mean value
df['Item_Weight'].fillna(df['Item_Weight'].mean() , inplace = True )

#checking for the missing values in "item_weight"
df['Item_Weight'].isnull().sum()

mode_of_outlet_size = df.pivot_table(values = 'Outlet_Size' ,  columns = 'Outlet_Type' , aggfunc = (lambda x: x.mode()[0]))

print(mode_of_outlet_size)

missing_values = df['Outlet_Size'].isnull()

print(missing_values)

df.loc[missing_values , 'Outlet_Size'] = df.loc[missing_values , 'Outlet_Type'].apply(lambda x: mode_of_outlet_size[x])

# checking for missing values
df.isnull().sum()

# taking some statisctical knowledge of the data
df.describe()

sns.set()

# plot for Item_weight distribution
plt.figure(figsize=(6,6))
sns.distplot(df['Item_Weight'])
plt.show()

# plot for Item_weight distribution
plt.figure(figsize=(6,6))
sns.distplot(df['Item_Visibility'])
plt.show()
#the data is presented as right skewed data or we can that data is positively skewed

# plot for Item_weight distribution
plt.figure(figsize=(6,6))
sns.distplot(df['Item_MRP'])
plt.show()

# plot for Item_weight distribution
plt.figure(figsize=(6,6))
sns.distplot(df['Item_Outlet_Sales'])
plt.show()

# Outlet establishment year column
plt.figure(figsize=(6,6))
sns.countplot( x='Outlet_Establishment_Year' , data=df )
plt.show()

# item fat contains
plt.figure(figsize=(6,6))
sns.countplot( x = 'Item_Fat_Content' , data=df)
plt.show()

plt.figure(figsize=(30,6))
sns.countplot( x = 'Item_Type' , data=df)
plt.show()

# Outlet_Size column
plt.figure(figsize=(6,6))
sns.countplot(x='Outlet_Size', data=df)
plt.show()

df.head()

df['Item_Fat_Content'].value_counts()

df.replace({'Item_Fat_Content': {'low fat':'Low Fat','LF':'Low Fat', 'reg':'Regular'}}, inplace=True)

df['Item_Fat_Content'].value_counts()

encoder = LabelEncoder()

df['Item_Identifier'] = encoder.fit_transform(df['Item_Identifier'])

df['Item_Fat_Content'] = encoder.fit_transform(df['Item_Fat_Content'])

df['Item_Type'] = encoder.fit_transform(df['Item_Type'])

df['Outlet_Identifier'] = encoder.fit_transform(df['Outlet_Identifier'])

df['Outlet_Size'] = encoder.fit_transform(df['Outlet_Size'])

df['Outlet_Location_Type'] = encoder.fit_transform(df['Outlet_Location_Type'])

df['Outlet_Type'] = encoder.fit_transform(df['Outlet_Type'])

df.head()

X = df.drop(columns = 'Item_Outlet_Sales' , axis=1)
Y = df['Item_Outlet_Sales']

print(X)

print(Y)

X_train , X_test , Y__train , Y_test = train_test_split(X , Y , test_size = 0.2 , random_state=0)

print(X.shape , X_train.shape , X_test.shape)

"""Machine Learning Model Training

XGBoost Regressor
"""

regressor  = XGBRegressor()

regressor.fit( X_train, Y__train)

# prediction on triaing data
training_data_prediction = regressor.predict(X_train)

# R squared Value
r2_train = metrics.r2_score(Y__train , training_data_prediction)

print("R squared value" , r2_train)

# prediction on test data
test_data_prediciton = regressor.predict(X_test)

# R squared Value
r2_test = metrics.r2_score(Y_test , test_data_prediciton)

print("R squared value" , r2_test)

df.info()

import pandas as pd
import datetime as dt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.metrics import accuracy_score, classification_report
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import precision_score, recall_score, f1_score
from surprise import Dataset, Reader, SVD
from surprise.model_selection import train_test_split as surprise_train_test_split
# from surprise.accuracy import precision_at_k, recall_at_k
from gensim.models import Word2Vec

!pip install numpy
!pip install scikit-surprise

data1=pd.read_excel("/content/CustomersData.xlsx")
data2=pd.read_csv("/content/Discount_Coupon.csv")
data3=pd.read_csv("/content/Marketing_Spend.csv")
data4=pd.read_csv("/content/Online_Sales.csv")
data5=pd.read_excel("/content/Tax_amount.xlsx")

data6=data1.merge(data4,on='CustomerID')

data7=data6.merge(data5,on='Product_Category')

import pandas as pd
import seaborn as sb
import numpy as np
import matplotlib.pyplot as plt
import warnings
import calendar
warnings.filterwarnings('ignore')
import datetime as dt
from sklearn.compose import make_column_selector
from matplotlib import cm
c1=cm.get_cmap('Accent')
c2=cm.get_cmap('twilight')

data2["Month"]=data2['Month'].apply(lambda x: dt.datetime.strptime(x, '%b').month)

data7=data7.merge(data3,left_on='Transaction_Date',right_on='Date')

data7.Transaction_Date=pd.to_datetime(data7.Transaction_Date,format='%m/%d/%Y')

data7['Month']=data7.Transaction_Date.apply(lambda x : x.strftime('%m'))

data7.Month=data7.Month.astype('int')

data7=data7.merge(data2,on=['Month','Product_Category'],how='outer')

data7=data7.dropna()

data7=data7.dropna()
data7.Month.info()

data7.duplicated().value_counts()

data7=data7.drop_duplicates()

data7['Total Prices']=data7.Avg_Price+data7.Delivery_Charges

data7.info()

data7

# Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import xgboost as xgb
from surprise import Dataset, Reader
from surprise.model_selection import train_test_split
from surprise import SVD
from surprise import accuracy
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam

# Load the dataset
data = data7

# Step 3: Personalized Recommendation with Generative AI (Collaborative Filtering)
from surprise import Dataset, Reader
from surprise.model_selection import train_test_split
from surprise import SVD
from surprise import accuracy

# Assuming you have user-item interaction data for recommendation
reader = Reader(rating_scale=(0, 5))
data_cf = Dataset.load_from_df(data[['CustomerID', 'Product_SKU', 'Quantity']], reader)

# Convert Surprise dataset to pandas DataFrame
data_cf_df = pd.DataFrame(data_cf.raw_ratings, columns=['CustomerID', 'Product_SKU', 'Quantity', 'Rating'])

# Split data into train and test sets
trainset, testset = train_test_split(data_cf, test_size=0.3, random_state=0)

# Train SVD algorithm (Collaborative Filtering)
algo_cf = SVD()
algo_cf.fit(trainset)

# Make predictions
predictions_cf = algo_cf.test(testset)

# Evaluate the model

# Example: Get personalized recommendations for a user
user_id = 17850.0  # Replace with the user ID you want recommendations for
items_to_recommend = []
for item_id in data['Product_SKU'].unique():
    if not data[(data['CustomerID'] == user_id) & (data['Product_SKU'] == item_id)].empty:
        continue
    predicted_rating = algo_cf.predict(user_id, item_id).est
    items_to_recommend.append((item_id, predicted_rating))

# Sort recommendations by predicted rating
items_to_recommend.sort(key=lambda x: x[1], reverse=True)

# Print top recommendations
top_n = 10  # Number of top recommendations to show
print(f"Top {top_n} Recommendations for User {user_id}:")
for item in items_to_recommend[:top_n]:
    print(f"Item SKU: {item[0]}")

val1=data[['CustomerID','Total Prices']]
x=val1.CustomerID.unique()[:20]
y=val1['Total Prices'].unique()[:20]
fig,axis=plt.subplots(figsize=(7,7))
val1=val1.sort_values(by='Total Prices',ascending=False)
sb.barplot(x=x,y=y,palette='bone_r',ax=axis)
plt.xticks(rotation=90)
plt.title("Costliest Purchase Id's")
plt.ylabel('Price')
plt.xlabel('Customer ID')

fig=plt.figure(figsize=(10,10))
axis=fig.add_axes([1,1,1,1])
val2=data.CustomerID.value_counts().sort_values(ascending=False).head(30)
sb.barplot(x=val2.index,y=val2,palette='copper',ax=axis)

plt.xticks(rotation=90)
plt.ylabel('No. of Purchases')
plt.title('Top 30 Popular Customer ID with Purchase Count')
for i in axis.patches:
    axis.annotate(i.get_height(),(i.get_x(),i.get_height()),va='bottom',ha='center')

val3=data.Gender.value_counts()
plt.pie(val3,labels=['Female','Male'],autopct="%1.1f%%",shadow=True,explode=(0.1,0),colors=[c2(0.7),c1(0.1)])
plt.axis('equal')
plt.title('Male - Female Purchase Comparision')
sb.set(style='white')

val4=data.Tenure_Months
sb.histplot(val4,kde=True,color=c1(0.3))
plt.xlabel('Months')
plt.ylabel('Frequency')
plt.title('Tenure Months Frequency of Customers')

freqdata=data.Product_Category.value_counts()
total=freqdata.sum()
percent=(freqdata.values/total)*100
sb.swarmplot(x=freqdata.index,y=percent,color=c1(0.8))
plt.xticks(rotation=85)
plt.title('Distribution of Category')
plt.ylabel('Percentages')

sb.histplot(data.Location,color=c2(0.5))
plt.ylabel('Frequency')
plt.xlabel('Cities')
plt.xticks(rotation=65)
plt.title('Location Frequencies')

sb.scatterplot(data=data,y='Delivery_Charges',x='Avg_Price',hue='GST',palette='viridis')
plt.title('Delivery Charges Vs Avg_Price')

data=data7
new=data[['Offline_Spend','Online_Spend','Month','Total Prices']].groupby('Month').sum()
mon=list(calendar.month_name)[1:]

sb.lineplot(y=new['Total Prices'],x=new.index,color=c1(0.1))
plt.xticks(new.index,mon,rotation=60)
plt.title("Total Spend With Month")

sb.lineplot(y=new.Online_Spend,x=new.index,color=c1(0.5))
plt.xticks(new.index,mon,rotation=60)
plt.title("Total Online Spend With Month")

sb.lineplot(y=new.Offline_Spend,x=new.index,color='orange')
plt.xticks(new.index,mon,rotation=60)
plt.title("Total Offline Spend With Month")

delivery=data[['Delivery_Charges','Month']].groupby('Month').sum()
sb.barplot(data=delivery,x=delivery.index,y='Delivery_Charges',palette='crest')
plt.xticks(range(12),mon,rotation=60)
plt.title("Total Delivery Charges per Month")